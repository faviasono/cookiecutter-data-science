{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML pipeline template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os\n",
    "import mlflow  # install\n",
    "import mlflow.azureml\n",
    "import azureml.core  # install\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import ComputeTarget\n",
    "\n",
    "from azureml.core import ScriptRunConfig\n",
    "import yaml\n",
    "from azureml.core import Dataset\n",
    "\n",
    "from azureml.core import Datastore\n",
    "from distutils.dir_util import copy_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Workspace - Info in .azureml/config.json file\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "print(\"MLflow version:\", mlflow.version.VERSION)\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep=\"\\n\")\n",
    "\n",
    "mlflow.set_tracking_uri(\n",
    "    ws.get_mlflow_tracking_uri()\n",
    ")  # integration of  MLFlow and AzureML - You need to install azureml-mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Experiment Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"azureml-pipeline\"\n",
    "mlflow.set_experiment(experiment_name)  # set an experiment name\n",
    "exp = Experiment(ws, experiment_name)  # create experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core import Dataset\n",
    "\n",
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep\n",
    "from azureml.pipeline.core import (\n",
    "    Pipeline,\n",
    "    PipelineData,\n",
    "    TrainingOutput,\n",
    "    PipelineParameter,\n",
    ")\n",
    "from azureml.data.output_dataset_config import OutputFileDatasetConfig\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Path from input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.datapath import DataPath, DataPathComputeBinding, DataReference\n",
    "\n",
    "# example\n",
    "path_on_datastore = \"hr_dataset\"\n",
    "data_path = DataPath(\n",
    "    datastore=ws.get_default_datastore(), path_on_datastore=path_on_datastore\n",
    ")\n",
    "data_path_param = PipelineParameter(name=\"representation\", default_value=data_path)\n",
    "datapath_input = (data_path_param, DataPathComputeBinding(mode=\"mount\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PythonScriptStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "base_image_name = \"sleepiz.azurecr.io/sleepiz-sag-dl-base:V1.1.3\"  # Define Docker image to use for each (multiple) steps\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = base_image_name\n",
    "run_config.environment.python.user_managed_dependencies = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset.get_by_name(ws, name='hr_dataset',version =1) # dataset to use\n",
    "\n",
    "augmented_data = OutputFileDatasetConfig(\n",
    "    name=\"output_path\", destination=(ws.get_default_datastore, \"results\")\n",
    ")  # reference to directory used between steps\n",
    "\n",
    "augmented_step = PythonScriptStep(\n",
    "    name=\"Name step\",\n",
    "    script_name=\"step_script.py\",  # file to use for preprocessing\n",
    "    source_directory=\"script_folder\",  # keep files in different directories to re-use steps that have not changed\n",
    "    compute_target=  ComputeTarget(ws, \"sag-cpu\"),  # target\n",
    "    arguments=[\n",
    "        \"--output_path\",\n",
    "        augmented_data,\n",
    "        \"--input-data\",\n",
    "        datapath_input,\n",
    "    ],  # where to save data\n",
    "    inputs=[datapath_input],  # load the dataset as input directory\n",
    "    allow_reuse=True,  # reuse if not changed occurred\n",
    "    runconfig=run_config,  # configuration (akin to Environment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperdrive Step\n",
    "1. define Paramter sampling strategy\n",
    "2. optimizing metric and direction\n",
    "3. Hyperdrive configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image_name = (\n",
    "    \"sleepiz.azurecr.io/sleepiz-sag-dl-base:V1.1.3\"  # Define Docker image to use\n",
    ")\n",
    "\n",
    "env_training = Environment(name=\"sleepiz\")\n",
    "env_training.docker.enabled = True\n",
    "env_training.docker.base_image = base_image_name\n",
    "env_training.python.user_managed_dependencies = True\n",
    "# new_env.python.interpreter_path = '/root/miniconda3/envs/deep_breath/bin/python' # remove env from docker img\n",
    "# new_env.python.conda_dependencies.add_pip_package('onnxruntime-gpu')\n",
    "env_training.environment_variables = {\"AZUREML_COMPUTE_USE_COMMON_RUNTIME\": \"false\"}\n",
    "\n",
    "training_data = augmented_data.as_input(\n",
    "    \"input_path\"\n",
    ")  # the output of the previous step become input of the following step\n",
    "\n",
    "# define training script and environment\n",
    "src = ScriptRunConfig(\n",
    "    source_directory=\"training/\",\n",
    "    script=\"train.py\",\n",
    "    arguments=[\"--input-data\", training_data],\n",
    "    environment=env_training,\n",
    "    compute_target=ComputeTarget(ws, name=\"sag-gpu\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import (\n",
    "    BayesianParameterSampling,\n",
    "    choice,\n",
    "    PrimaryMetricGoal,\n",
    "    HyperDriveRun,\n",
    "    HyperDriveConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sampling = BayesianParameterSampling(\n",
    "    {\n",
    "        \"--dense-units\": choice(5, 10, 20),\n",
    "        \"--n-epochs\": choice(2),\n",
    "    }\n",
    ")\n",
    "\n",
    "primary_metric_name = \"prc\"  # name to monitor (ideally should be validation_prc)\n",
    "primary_metric_goal = PrimaryMetricGoal.MAXIMIZE  # direction of optimization\n",
    "\n",
    "max_total_runs = 2  # total runs for hp tuning\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(\n",
    "    run_config=src,  # ScriptRunConfig file with training script\n",
    "    hyperparameter_sampling=param_sampling,  # parameters space\n",
    "    policy=None,\n",
    "    primary_metric_name=primary_metric_name,\n",
    "    primary_metric_goal=primary_metric_goal,\n",
    "    max_total_runs=max_total_runs,\n",
    "    max_concurrent_runs=2,\n",
    "    # must not be high, othewise Bayesian sampling can't take info from the past\n",
    "    # start from last iteration of the parent (need same HP space),\n",
    "    # esume_child_runs =\n",
    ")\n",
    "\n",
    "\n",
    "# Save metric to blob storage to automate saving of the best model - Not working at the moment\n",
    "metrics_output_name = \"metrics_output\"\n",
    "metrics_output = PipelineData(\"metrics_output\", datastore=ws.get_default_datastore())\n",
    "\n",
    "# Save models to blob storage to automate saving of the best model found\n",
    "model_output_name = \"model_output\"\n",
    "saved_model = PipelineData(\n",
    "    name=\"saved_model\",\n",
    "    datastore=ws.get_default_datastore(),\n",
    "    pipeline_output_name=model_output_name,\n",
    "    training_output=TrainingOutput(\n",
    "        \"Model\", metric=\"prc\", model_file=\"./outputs/model/model.h5\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Hyperdrive Step\n",
    "hd_step = HyperDriveStep(\n",
    "    name=\"HP tuning\",\n",
    "    hyperdrive_config=hyperdrive_config,\n",
    "    inputs=[training_data],  # input\n",
    "    outputs=[saved_model],  # output\n",
    "    metrics_output=metrics_output,\n",
    "    allow_reuse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep\n",
    "from azureml.pipeline.core import (\n",
    "    Pipeline,\n",
    "    PipelineData,\n",
    "    TrainingOutput,\n",
    "    PipelineParameter,\n",
    "    PipelineRun,\n",
    ")\n",
    "from azureml.data.output_dataset_config import OutputFileDatasetConfig\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    ws, [augmented_step, hd_step]\n",
    ")  # Create pipeline\n",
    "\n",
    "pipeline.validate()  # validate pipeline\n",
    "\n",
    "pipeline_run = exp.submit(pipeline, show_output=True)  # Submit pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details see the WIKI [page]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db756e184795c6d87b8dda5c6cd36f5d6415ef0ab36e168aae08f64fba1e2367"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('azureml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
